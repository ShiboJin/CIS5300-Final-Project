{
  "best_global_step": 150,
  "best_metric": 0.2576937675476074,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 50,
  "global_step": 159,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.019002375296912115,
      "grad_norm": 0.5867632031440735,
      "learning_rate": 0.0,
      "loss": 0.5484,
      "step": 1
    },
    {
      "epoch": 0.03800475059382423,
      "grad_norm": 0.5956152081489563,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.4929,
      "step": 2
    },
    {
      "epoch": 0.057007125890736345,
      "grad_norm": 0.6229776740074158,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.5579,
      "step": 3
    },
    {
      "epoch": 0.07600950118764846,
      "grad_norm": 0.607339084148407,
      "learning_rate": 1.5e-06,
      "loss": 0.5286,
      "step": 4
    },
    {
      "epoch": 0.09501187648456057,
      "grad_norm": 0.6152262091636658,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.5231,
      "step": 5
    },
    {
      "epoch": 0.11401425178147269,
      "grad_norm": 0.6194092631340027,
      "learning_rate": 2.5e-06,
      "loss": 0.5423,
      "step": 6
    },
    {
      "epoch": 0.1330166270783848,
      "grad_norm": 0.5951664447784424,
      "learning_rate": 3e-06,
      "loss": 0.5239,
      "step": 7
    },
    {
      "epoch": 0.15201900237529692,
      "grad_norm": 0.6345670223236084,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.5144,
      "step": 8
    },
    {
      "epoch": 0.171021377672209,
      "grad_norm": 0.6299501061439514,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.5135,
      "step": 9
    },
    {
      "epoch": 0.19002375296912113,
      "grad_norm": 0.6316357254981995,
      "learning_rate": 4.5e-06,
      "loss": 0.5235,
      "step": 10
    },
    {
      "epoch": 0.20902612826603326,
      "grad_norm": 0.5887584686279297,
      "learning_rate": 5e-06,
      "loss": 0.5322,
      "step": 11
    },
    {
      "epoch": 0.22802850356294538,
      "grad_norm": 0.6099128127098083,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.5106,
      "step": 12
    },
    {
      "epoch": 0.24703087885985747,
      "grad_norm": 0.5971607565879822,
      "learning_rate": 6e-06,
      "loss": 0.5148,
      "step": 13
    },
    {
      "epoch": 0.2660332541567696,
      "grad_norm": 0.6311839818954468,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.5147,
      "step": 14
    },
    {
      "epoch": 0.2850356294536817,
      "grad_norm": 0.6359332203865051,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.541,
      "step": 15
    },
    {
      "epoch": 0.30403800475059384,
      "grad_norm": 0.6506993770599365,
      "learning_rate": 7.5e-06,
      "loss": 0.544,
      "step": 16
    },
    {
      "epoch": 0.32304038004750596,
      "grad_norm": 0.6006838083267212,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.5117,
      "step": 17
    },
    {
      "epoch": 0.342042755344418,
      "grad_norm": 0.6553504467010498,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.5295,
      "step": 18
    },
    {
      "epoch": 0.36104513064133015,
      "grad_norm": 0.6058700680732727,
      "learning_rate": 9e-06,
      "loss": 0.5548,
      "step": 19
    },
    {
      "epoch": 0.38004750593824227,
      "grad_norm": 0.6243535280227661,
      "learning_rate": 9.5e-06,
      "loss": 0.5008,
      "step": 20
    },
    {
      "epoch": 0.3990498812351544,
      "grad_norm": 0.6326888203620911,
      "learning_rate": 1e-05,
      "loss": 0.5263,
      "step": 21
    },
    {
      "epoch": 0.4180522565320665,
      "grad_norm": 0.6250186562538147,
      "learning_rate": 1.05e-05,
      "loss": 0.5039,
      "step": 22
    },
    {
      "epoch": 0.43705463182897863,
      "grad_norm": 0.577439546585083,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.5008,
      "step": 23
    },
    {
      "epoch": 0.45605700712589076,
      "grad_norm": 0.6485589146614075,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.5285,
      "step": 24
    },
    {
      "epoch": 0.4750593824228028,
      "grad_norm": 0.644705057144165,
      "learning_rate": 1.2e-05,
      "loss": 0.5167,
      "step": 25
    },
    {
      "epoch": 0.49406175771971494,
      "grad_norm": 0.6431676149368286,
      "learning_rate": 1.25e-05,
      "loss": 0.507,
      "step": 26
    },
    {
      "epoch": 0.5130641330166271,
      "grad_norm": 0.6372535824775696,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.5262,
      "step": 27
    },
    {
      "epoch": 0.5320665083135392,
      "grad_norm": 0.6646082997322083,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.5246,
      "step": 28
    },
    {
      "epoch": 0.5510688836104513,
      "grad_norm": 0.6208427548408508,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.5083,
      "step": 29
    },
    {
      "epoch": 0.5700712589073634,
      "grad_norm": 0.6636674404144287,
      "learning_rate": 1.45e-05,
      "loss": 0.5424,
      "step": 30
    },
    {
      "epoch": 0.5890736342042755,
      "grad_norm": 0.607780396938324,
      "learning_rate": 1.5e-05,
      "loss": 0.4874,
      "step": 31
    },
    {
      "epoch": 0.6080760095011877,
      "grad_norm": 0.6551379561424255,
      "learning_rate": 1.55e-05,
      "loss": 0.521,
      "step": 32
    },
    {
      "epoch": 0.6270783847980997,
      "grad_norm": 0.6367189884185791,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.4848,
      "step": 33
    },
    {
      "epoch": 0.6460807600950119,
      "grad_norm": 0.6279333233833313,
      "learning_rate": 1.65e-05,
      "loss": 0.4937,
      "step": 34
    },
    {
      "epoch": 0.665083135391924,
      "grad_norm": 0.6199156045913696,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.5317,
      "step": 35
    },
    {
      "epoch": 0.684085510688836,
      "grad_norm": 0.6376914978027344,
      "learning_rate": 1.75e-05,
      "loss": 0.4977,
      "step": 36
    },
    {
      "epoch": 0.7030878859857482,
      "grad_norm": 0.6252239346504211,
      "learning_rate": 1.8e-05,
      "loss": 0.4852,
      "step": 37
    },
    {
      "epoch": 0.7220902612826603,
      "grad_norm": 0.6837960481643677,
      "learning_rate": 1.85e-05,
      "loss": 0.5007,
      "step": 38
    },
    {
      "epoch": 0.7410926365795725,
      "grad_norm": 0.6354016661643982,
      "learning_rate": 1.9e-05,
      "loss": 0.4769,
      "step": 39
    },
    {
      "epoch": 0.7600950118764845,
      "grad_norm": 0.6449388265609741,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.486,
      "step": 40
    },
    {
      "epoch": 0.7790973871733967,
      "grad_norm": 0.6040897965431213,
      "learning_rate": 2e-05,
      "loss": 0.4747,
      "step": 41
    },
    {
      "epoch": 0.7980997624703088,
      "grad_norm": 0.6441868543624878,
      "learning_rate": 2.05e-05,
      "loss": 0.4836,
      "step": 42
    },
    {
      "epoch": 0.8171021377672208,
      "grad_norm": 0.6444278359413147,
      "learning_rate": 2.1e-05,
      "loss": 0.464,
      "step": 43
    },
    {
      "epoch": 0.836104513064133,
      "grad_norm": 0.635764479637146,
      "learning_rate": 2.15e-05,
      "loss": 0.4364,
      "step": 44
    },
    {
      "epoch": 0.8551068883610451,
      "grad_norm": 0.686348557472229,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.4685,
      "step": 45
    },
    {
      "epoch": 0.8741092636579573,
      "grad_norm": 0.6633545160293579,
      "learning_rate": 2.25e-05,
      "loss": 0.4824,
      "step": 46
    },
    {
      "epoch": 0.8931116389548693,
      "grad_norm": 0.6351617574691772,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.4332,
      "step": 47
    },
    {
      "epoch": 0.9121140142517815,
      "grad_norm": 0.6523377895355225,
      "learning_rate": 2.35e-05,
      "loss": 0.4692,
      "step": 48
    },
    {
      "epoch": 0.9311163895486936,
      "grad_norm": 0.6274073123931885,
      "learning_rate": 2.4e-05,
      "loss": 0.4288,
      "step": 49
    },
    {
      "epoch": 0.9501187648456056,
      "grad_norm": 0.68719083070755,
      "learning_rate": 2.45e-05,
      "loss": 0.4563,
      "step": 50
    },
    {
      "epoch": 0.9501187648456056,
      "eval_loss": 0.4151036739349365,
      "eval_runtime": 7.0638,
      "eval_samples_per_second": 105.892,
      "eval_steps_per_second": 6.654,
      "step": 50
    },
    {
      "epoch": 0.9691211401425178,
      "grad_norm": 0.6446920037269592,
      "learning_rate": 2.5e-05,
      "loss": 0.4196,
      "step": 51
    },
    {
      "epoch": 0.9881235154394299,
      "grad_norm": 0.6136021614074707,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.4026,
      "step": 52
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7040685415267944,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.4191,
      "step": 53
    },
    {
      "epoch": 1.0190023752969122,
      "grad_norm": 0.5715194940567017,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.4184,
      "step": 54
    },
    {
      "epoch": 1.0380047505938241,
      "grad_norm": 0.6348327398300171,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.4367,
      "step": 55
    },
    {
      "epoch": 1.0570071258907363,
      "grad_norm": 0.5746062397956848,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.3808,
      "step": 56
    },
    {
      "epoch": 1.0760095011876485,
      "grad_norm": 0.6051304936408997,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.3815,
      "step": 57
    },
    {
      "epoch": 1.0950118764845607,
      "grad_norm": 0.5857986807823181,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.427,
      "step": 58
    },
    {
      "epoch": 1.1140142517814726,
      "grad_norm": 0.5236419439315796,
      "learning_rate": 2.9e-05,
      "loss": 0.3717,
      "step": 59
    },
    {
      "epoch": 1.1330166270783848,
      "grad_norm": 0.5261432528495789,
      "learning_rate": 2.95e-05,
      "loss": 0.3822,
      "step": 60
    },
    {
      "epoch": 1.152019002375297,
      "grad_norm": 0.5523014068603516,
      "learning_rate": 3e-05,
      "loss": 0.3738,
      "step": 61
    },
    {
      "epoch": 1.171021377672209,
      "grad_norm": 0.46428388357162476,
      "learning_rate": 3.05e-05,
      "loss": 0.4005,
      "step": 62
    },
    {
      "epoch": 1.190023752969121,
      "grad_norm": 0.43731042742729187,
      "learning_rate": 3.1e-05,
      "loss": 0.3681,
      "step": 63
    },
    {
      "epoch": 1.2090261282660333,
      "grad_norm": 0.4326552152633667,
      "learning_rate": 3.15e-05,
      "loss": 0.374,
      "step": 64
    },
    {
      "epoch": 1.2280285035629455,
      "grad_norm": 0.42963212728500366,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.3675,
      "step": 65
    },
    {
      "epoch": 1.2470308788598574,
      "grad_norm": 0.42684972286224365,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.3578,
      "step": 66
    },
    {
      "epoch": 1.2660332541567696,
      "grad_norm": 0.36879298090934753,
      "learning_rate": 3.3e-05,
      "loss": 0.3664,
      "step": 67
    },
    {
      "epoch": 1.2850356294536818,
      "grad_norm": 0.3620147109031677,
      "learning_rate": 3.35e-05,
      "loss": 0.3597,
      "step": 68
    },
    {
      "epoch": 1.304038004750594,
      "grad_norm": 0.33578673005104065,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.3381,
      "step": 69
    },
    {
      "epoch": 1.323040380047506,
      "grad_norm": 0.34143978357315063,
      "learning_rate": 3.45e-05,
      "loss": 0.3496,
      "step": 70
    },
    {
      "epoch": 1.342042755344418,
      "grad_norm": 0.317973792552948,
      "learning_rate": 3.5e-05,
      "loss": 0.3327,
      "step": 71
    },
    {
      "epoch": 1.36104513064133,
      "grad_norm": 0.327708899974823,
      "learning_rate": 3.55e-05,
      "loss": 0.363,
      "step": 72
    },
    {
      "epoch": 1.3800475059382422,
      "grad_norm": 0.33309808373451233,
      "learning_rate": 3.6e-05,
      "loss": 0.3545,
      "step": 73
    },
    {
      "epoch": 1.3990498812351544,
      "grad_norm": 0.30325838923454285,
      "learning_rate": 3.65e-05,
      "loss": 0.3445,
      "step": 74
    },
    {
      "epoch": 1.4180522565320666,
      "grad_norm": 0.33115828037261963,
      "learning_rate": 3.7e-05,
      "loss": 0.3301,
      "step": 75
    },
    {
      "epoch": 1.4370546318289787,
      "grad_norm": 0.288194864988327,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.3196,
      "step": 76
    },
    {
      "epoch": 1.4560570071258907,
      "grad_norm": 0.2806020677089691,
      "learning_rate": 3.8e-05,
      "loss": 0.3393,
      "step": 77
    },
    {
      "epoch": 1.4750593824228029,
      "grad_norm": 0.2810139060020447,
      "learning_rate": 3.85e-05,
      "loss": 0.3249,
      "step": 78
    },
    {
      "epoch": 1.4940617577197148,
      "grad_norm": 0.27332836389541626,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.3314,
      "step": 79
    },
    {
      "epoch": 1.513064133016627,
      "grad_norm": 0.2614637613296509,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.3393,
      "step": 80
    },
    {
      "epoch": 1.5320665083135392,
      "grad_norm": 0.22609902918338776,
      "learning_rate": 4e-05,
      "loss": 0.3178,
      "step": 81
    },
    {
      "epoch": 1.5510688836104514,
      "grad_norm": 0.23060204088687897,
      "learning_rate": 4.05e-05,
      "loss": 0.3242,
      "step": 82
    },
    {
      "epoch": 1.5700712589073635,
      "grad_norm": 0.23281535506248474,
      "learning_rate": 4.1e-05,
      "loss": 0.2991,
      "step": 83
    },
    {
      "epoch": 1.5890736342042755,
      "grad_norm": 0.24176466464996338,
      "learning_rate": 4.15e-05,
      "loss": 0.3182,
      "step": 84
    },
    {
      "epoch": 1.6080760095011877,
      "grad_norm": 0.25058987736701965,
      "learning_rate": 4.2e-05,
      "loss": 0.3075,
      "step": 85
    },
    {
      "epoch": 1.6270783847980996,
      "grad_norm": 0.23504877090454102,
      "learning_rate": 4.25e-05,
      "loss": 0.3373,
      "step": 86
    },
    {
      "epoch": 1.6460807600950118,
      "grad_norm": 0.23706060647964478,
      "learning_rate": 4.3e-05,
      "loss": 0.306,
      "step": 87
    },
    {
      "epoch": 1.665083135391924,
      "grad_norm": 0.21564048528671265,
      "learning_rate": 4.35e-05,
      "loss": 0.317,
      "step": 88
    },
    {
      "epoch": 1.6840855106888362,
      "grad_norm": 0.219890758395195,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.3028,
      "step": 89
    },
    {
      "epoch": 1.7030878859857483,
      "grad_norm": 0.21511535346508026,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.3328,
      "step": 90
    },
    {
      "epoch": 1.7220902612826603,
      "grad_norm": 0.21360372006893158,
      "learning_rate": 4.5e-05,
      "loss": 0.3178,
      "step": 91
    },
    {
      "epoch": 1.7410926365795725,
      "grad_norm": 0.1866035908460617,
      "learning_rate": 4.55e-05,
      "loss": 0.2826,
      "step": 92
    },
    {
      "epoch": 1.7600950118764844,
      "grad_norm": 0.21181006729602814,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.2859,
      "step": 93
    },
    {
      "epoch": 1.7790973871733966,
      "grad_norm": 0.20600625872612,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.3292,
      "step": 94
    },
    {
      "epoch": 1.7980997624703088,
      "grad_norm": 0.1897069215774536,
      "learning_rate": 4.7e-05,
      "loss": 0.3015,
      "step": 95
    },
    {
      "epoch": 1.817102137767221,
      "grad_norm": 0.18876852095127106,
      "learning_rate": 4.75e-05,
      "loss": 0.3063,
      "step": 96
    },
    {
      "epoch": 1.8361045130641331,
      "grad_norm": 0.17265059053897858,
      "learning_rate": 4.8e-05,
      "loss": 0.2895,
      "step": 97
    },
    {
      "epoch": 1.855106888361045,
      "grad_norm": 0.19541485607624054,
      "learning_rate": 4.85e-05,
      "loss": 0.2964,
      "step": 98
    },
    {
      "epoch": 1.8741092636579573,
      "grad_norm": 0.18278175592422485,
      "learning_rate": 4.9e-05,
      "loss": 0.3133,
      "step": 99
    },
    {
      "epoch": 1.8931116389548692,
      "grad_norm": 0.17330187559127808,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.2925,
      "step": 100
    },
    {
      "epoch": 1.8931116389548692,
      "eval_loss": 0.2771315276622772,
      "eval_runtime": 7.1038,
      "eval_samples_per_second": 105.296,
      "eval_steps_per_second": 6.616,
      "step": 100
    },
    {
      "epoch": 1.9121140142517814,
      "grad_norm": 0.17646823823451996,
      "learning_rate": 5e-05,
      "loss": 0.2843,
      "step": 101
    },
    {
      "epoch": 1.9311163895486936,
      "grad_norm": 0.19332486391067505,
      "learning_rate": 4.996456739191905e-05,
      "loss": 0.2907,
      "step": 102
    },
    {
      "epoch": 1.9501187648456058,
      "grad_norm": 0.169064000248909,
      "learning_rate": 4.985837000525343e-05,
      "loss": 0.296,
      "step": 103
    },
    {
      "epoch": 1.969121140142518,
      "grad_norm": 0.15975794196128845,
      "learning_rate": 4.9681708868033616e-05,
      "loss": 0.2829,
      "step": 104
    },
    {
      "epoch": 1.98812351543943,
      "grad_norm": 0.15336745977401733,
      "learning_rate": 4.9435084745446666e-05,
      "loss": 0.2834,
      "step": 105
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.16875296831130981,
      "learning_rate": 4.91191967203629e-05,
      "loss": 0.3026,
      "step": 106
    },
    {
      "epoch": 2.019002375296912,
      "grad_norm": 0.16360647976398468,
      "learning_rate": 4.873494021170953e-05,
      "loss": 0.2822,
      "step": 107
    },
    {
      "epoch": 2.0380047505938244,
      "grad_norm": 0.15373718738555908,
      "learning_rate": 4.8283404436308464e-05,
      "loss": 0.2873,
      "step": 108
    },
    {
      "epoch": 2.0570071258907365,
      "grad_norm": 0.1524592489004135,
      "learning_rate": 4.7765869321372836e-05,
      "loss": 0.2777,
      "step": 109
    },
    {
      "epoch": 2.0760095011876483,
      "grad_norm": 0.14981696009635925,
      "learning_rate": 4.7183801876414294e-05,
      "loss": 0.2899,
      "step": 110
    },
    {
      "epoch": 2.0950118764845604,
      "grad_norm": 0.15118953585624695,
      "learning_rate": 4.653885203484515e-05,
      "loss": 0.3056,
      "step": 111
    },
    {
      "epoch": 2.1140142517814726,
      "grad_norm": 0.13754263520240784,
      "learning_rate": 4.5832847977062874e-05,
      "loss": 0.2876,
      "step": 112
    },
    {
      "epoch": 2.133016627078385,
      "grad_norm": 0.1476159542798996,
      "learning_rate": 4.5067790948274094e-05,
      "loss": 0.2798,
      "step": 113
    },
    {
      "epoch": 2.152019002375297,
      "grad_norm": 0.16173484921455383,
      "learning_rate": 4.4245849585747654e-05,
      "loss": 0.2851,
      "step": 114
    },
    {
      "epoch": 2.171021377672209,
      "grad_norm": 0.1225460022687912,
      "learning_rate": 4.336935377157668e-05,
      "loss": 0.277,
      "step": 115
    },
    {
      "epoch": 2.1900237529691213,
      "grad_norm": 0.1399509310722351,
      "learning_rate": 4.2440788028374624e-05,
      "loss": 0.2886,
      "step": 116
    },
    {
      "epoch": 2.209026128266033,
      "grad_norm": 0.1391281634569168,
      "learning_rate": 4.146278447662597e-05,
      "loss": 0.2773,
      "step": 117
    },
    {
      "epoch": 2.2280285035629452,
      "grad_norm": 0.14490167796611786,
      "learning_rate": 4.04381153736548e-05,
      "loss": 0.2898,
      "step": 118
    },
    {
      "epoch": 2.2470308788598574,
      "grad_norm": 0.14378255605697632,
      "learning_rate": 3.9369685255360175e-05,
      "loss": 0.2752,
      "step": 119
    },
    {
      "epoch": 2.2660332541567696,
      "grad_norm": 0.15317343175411224,
      "learning_rate": 3.826052270299356e-05,
      "loss": 0.2678,
      "step": 120
    },
    {
      "epoch": 2.2850356294536818,
      "grad_norm": 0.145989790558815,
      "learning_rate": 3.711377175831626e-05,
      "loss": 0.27,
      "step": 121
    },
    {
      "epoch": 2.304038004750594,
      "grad_norm": 0.12673306465148926,
      "learning_rate": 3.593268301147139e-05,
      "loss": 0.2689,
      "step": 122
    },
    {
      "epoch": 2.323040380047506,
      "grad_norm": 0.130817711353302,
      "learning_rate": 3.472060438683302e-05,
      "loss": 0.2607,
      "step": 123
    },
    {
      "epoch": 2.342042755344418,
      "grad_norm": 0.13872741162776947,
      "learning_rate": 3.348097165295076e-05,
      "loss": 0.2706,
      "step": 124
    },
    {
      "epoch": 2.36104513064133,
      "grad_norm": 0.1365336924791336,
      "learning_rate": 3.2217298683490525e-05,
      "loss": 0.2831,
      "step": 125
    },
    {
      "epoch": 2.380047505938242,
      "grad_norm": 0.13266310095787048,
      "learning_rate": 3.093316749677788e-05,
      "loss": 0.278,
      "step": 126
    },
    {
      "epoch": 2.3990498812351544,
      "grad_norm": 0.1398342251777649,
      "learning_rate": 2.9632218102177862e-05,
      "loss": 0.2933,
      "step": 127
    },
    {
      "epoch": 2.4180522565320666,
      "grad_norm": 0.12314852327108383,
      "learning_rate": 2.8318138182093052e-05,
      "loss": 0.2839,
      "step": 128
    },
    {
      "epoch": 2.4370546318289787,
      "grad_norm": 0.1552334576845169,
      "learning_rate": 2.6994652638827078e-05,
      "loss": 0.3087,
      "step": 129
    },
    {
      "epoch": 2.456057007125891,
      "grad_norm": 0.12615050375461578,
      "learning_rate": 2.566551303594437e-05,
      "loss": 0.2773,
      "step": 130
    },
    {
      "epoch": 2.4750593824228027,
      "grad_norm": 0.1359056681394577,
      "learning_rate": 2.433448696405563e-05,
      "loss": 0.289,
      "step": 131
    },
    {
      "epoch": 2.494061757719715,
      "grad_norm": 0.12710754573345184,
      "learning_rate": 2.300534736117292e-05,
      "loss": 0.2726,
      "step": 132
    },
    {
      "epoch": 2.513064133016627,
      "grad_norm": 0.13185855746269226,
      "learning_rate": 2.1681861817906954e-05,
      "loss": 0.2531,
      "step": 133
    },
    {
      "epoch": 2.532066508313539,
      "grad_norm": 0.1311415731906891,
      "learning_rate": 2.0367781897822147e-05,
      "loss": 0.2558,
      "step": 134
    },
    {
      "epoch": 2.5510688836104514,
      "grad_norm": 0.12899599969387054,
      "learning_rate": 1.9066832503222128e-05,
      "loss": 0.27,
      "step": 135
    },
    {
      "epoch": 2.5700712589073635,
      "grad_norm": 0.1338449865579605,
      "learning_rate": 1.778270131650948e-05,
      "loss": 0.2894,
      "step": 136
    },
    {
      "epoch": 2.5890736342042757,
      "grad_norm": 0.12651017308235168,
      "learning_rate": 1.651902834704924e-05,
      "loss": 0.2589,
      "step": 137
    },
    {
      "epoch": 2.608076009501188,
      "grad_norm": 0.12448780238628387,
      "learning_rate": 1.5279395613166986e-05,
      "loss": 0.2671,
      "step": 138
    },
    {
      "epoch": 2.6270783847980996,
      "grad_norm": 0.1371690034866333,
      "learning_rate": 1.4067316988528617e-05,
      "loss": 0.2973,
      "step": 139
    },
    {
      "epoch": 2.646080760095012,
      "grad_norm": 0.1221197172999382,
      "learning_rate": 1.2886228241683749e-05,
      "loss": 0.2609,
      "step": 140
    },
    {
      "epoch": 2.665083135391924,
      "grad_norm": 0.1499190628528595,
      "learning_rate": 1.173947729700644e-05,
      "loss": 0.2691,
      "step": 141
    },
    {
      "epoch": 2.684085510688836,
      "grad_norm": 0.13231539726257324,
      "learning_rate": 1.063031474463983e-05,
      "loss": 0.2808,
      "step": 142
    },
    {
      "epoch": 2.7030878859857483,
      "grad_norm": 0.16001398861408234,
      "learning_rate": 9.561884626345205e-06,
      "loss": 0.258,
      "step": 143
    },
    {
      "epoch": 2.72209026128266,
      "grad_norm": 0.15234912931919098,
      "learning_rate": 8.537215523374038e-06,
      "loss": 0.2818,
      "step": 144
    },
    {
      "epoch": 2.7410926365795723,
      "grad_norm": 0.1283213049173355,
      "learning_rate": 7.5592119716253855e-06,
      "loss": 0.2675,
      "step": 145
    },
    {
      "epoch": 2.7600950118764844,
      "grad_norm": 0.1211400106549263,
      "learning_rate": 6.6306462284233234e-06,
      "loss": 0.2699,
      "step": 146
    },
    {
      "epoch": 2.7790973871733966,
      "grad_norm": 0.13811878859996796,
      "learning_rate": 5.75415041425234e-06,
      "loss": 0.259,
      "step": 147
    },
    {
      "epoch": 2.798099762470309,
      "grad_norm": 0.1328338384628296,
      "learning_rate": 4.932209051725914e-06,
      "loss": 0.2707,
      "step": 148
    },
    {
      "epoch": 2.817102137767221,
      "grad_norm": 0.1436322182416916,
      "learning_rate": 4.167152022937124e-06,
      "loss": 0.2771,
      "step": 149
    },
    {
      "epoch": 2.836104513064133,
      "grad_norm": 0.11910469084978104,
      "learning_rate": 3.4611479651548457e-06,
      "loss": 0.2658,
      "step": 150
    },
    {
      "epoch": 2.836104513064133,
      "eval_loss": 0.2576937675476074,
      "eval_runtime": 7.1002,
      "eval_samples_per_second": 105.35,
      "eval_steps_per_second": 6.62,
      "step": 150
    },
    {
      "epoch": 2.8551068883610453,
      "grad_norm": 0.13979293406009674,
      "learning_rate": 2.8161981235857143e-06,
      "loss": 0.2674,
      "step": 151
    },
    {
      "epoch": 2.8741092636579575,
      "grad_norm": 0.13686811923980713,
      "learning_rate": 2.2341306786271695e-06,
      "loss": 0.2733,
      "step": 152
    },
    {
      "epoch": 2.8931116389548692,
      "grad_norm": 0.13106566667556763,
      "learning_rate": 1.7165955636915392e-06,
      "loss": 0.2606,
      "step": 153
    },
    {
      "epoch": 2.9121140142517814,
      "grad_norm": 0.12689447402954102,
      "learning_rate": 1.265059788290468e-06,
      "loss": 0.2634,
      "step": 154
    },
    {
      "epoch": 2.9311163895486936,
      "grad_norm": 0.12668976187705994,
      "learning_rate": 8.808032796371019e-07,
      "loss": 0.2605,
      "step": 155
    },
    {
      "epoch": 2.9501187648456058,
      "grad_norm": 0.12443191558122635,
      "learning_rate": 5.649152545533332e-07,
      "loss": 0.2724,
      "step": 156
    },
    {
      "epoch": 2.969121140142518,
      "grad_norm": 0.139907106757164,
      "learning_rate": 3.1829113196638614e-07,
      "loss": 0.2791,
      "step": 157
    },
    {
      "epoch": 2.9881235154394297,
      "grad_norm": 0.13866697251796722,
      "learning_rate": 1.4162999474657268e-07,
      "loss": 0.2728,
      "step": 158
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.17040061950683594,
      "learning_rate": 3.543260808095139e-08,
      "loss": 0.2941,
      "step": 159
    },
    {
      "epoch": 3.0,
      "step": 159,
      "total_flos": 4.617635793941299e+16,
      "train_loss": 0.3703381640356292,
      "train_runtime": 533.4619,
      "train_samples_per_second": 37.819,
      "train_steps_per_second": 0.298
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 159,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.617635793941299e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
